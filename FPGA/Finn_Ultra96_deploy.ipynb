{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup to allow resume at any step\n",
    "#Note: Copy tmp folder to retain data\n",
    "#To start running from any cells, run this setup cell first\n",
    "#Necessary\n",
    "import inspect\n",
    "import netron\n",
    "from finn.util.basic import make_build_dir\n",
    "from IPython.display import IFrame\n",
    "import onnx\n",
    "import brevitas.onnx as bo\n",
    "from finn.util.basic import pynq_part_map\n",
    "from pkgutil import get_data\n",
    "import onnx.numpy_helper as nph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "#API for model tidy up and HLS synthesis\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.codegen_ipgen import CodeGen_ipgen\n",
    "from finn.transformation.fpgadataflow.hlssynth_ipgen import HLSSynth_IPGen\n",
    "from finn.transformation.fpgadataflow.codegen_ipstitch import CodeGen_ipstitch\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import ReplaceVerilogRelPaths\n",
    "from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "#For pretrained network\n",
    "from pkgutil import get_data\n",
    "from finn.util.test import get_test_model_trained\n",
    "\n",
    "#Code inspecction for debugging\n",
    "def showSrc(what):\n",
    "    print(\"\".join(inspect.getsourcelines(what)[0]))\n",
    "    \n",
    "#Vizulization for design recheck\n",
    "def showInNetron(model_filename):\n",
    "    netron.start(model_filename, port=8081, host=\"0.0.0.0\")\n",
    "    return IFrame(src=\"http://0.0.0.0:8081/\", width=\"100%\", height=400)\n",
    "\n",
    "#All HLS files location: Workspace default is mounted to /tmp/\n",
    "build_dir = \"/workspace/finn\"\n",
    "model_name = build_dir + \"/fpga_model\"\n",
    "model_extension = \".onnx\"\n",
    "\n",
    "#Board definition\n",
    "pynq_board = \"Ultra96\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 5\n",
    "ip = \"192.168.3.1\"\n",
    "username = \"xilinx\"\n",
    "password = \"xilinx\"\n",
    "target_dir = \"/home/xilinx/dance_dance\"\n",
    "\n",
    "#all intermediate model \n",
    "model_name_original = model_name + model_extension\n",
    "model_name_tidy = model_name + \"_tidy\" +  model_extension\n",
    "model_name_streamlined = model_name + \"_streamlined\" +  model_extension\n",
    "model_name_hls_ready = model_name + \"_hls_ready\" +  model_extension\n",
    "model_name_hls_layers = model_name + \"_hls_layers\" +  model_extension\n",
    "model_name_data_flow = model_name + \"_dataflow_parent\" +  model_extension \n",
    "model_name_set_folding = model_name + \"_set_folding\" + model_extension\n",
    "model_name_ipgen = model_name + \"_ipgen\" + model_extension\n",
    "model_name_ipstitch = model_name + \"_ipstitch\" + model_extension\n",
    "model_name_pynq_proj = model_name + \"_pynq_project\" + model_extension\n",
    "model_name_post_synthesis = model_name + \"_post_synthesis\" + model_extension\n",
    "model_name_deploy = model_name + \"_deploy\" + model_extension\n",
    "model_name_deploy_integrated = model_name + \"_deploy_integrated\" + model_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and weight\n",
    "showInNetron(model_name_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model tidy up\n",
    "model = ModelWrapper(model_name_original)\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "\n",
    "model.save(model_name_tidy)\n",
    "showInNetron(model_name_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model streamline. Convert multtplication, add, subtraction, and collapse them to parallel threshold\n",
    "model = ModelWrapper(model_name_tidy)\n",
    "model = model.transform(Streamline())\n",
    "model.save(model_name_streamlined)\n",
    "showInNetron(model_name_streamlined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model HLS conversion for Bipolar binary model\n",
    "#Should not collapse using this Bipolar transformation \n",
    "\n",
    "model = ModelWrapper(model_name_streamlined)\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "model.save(model_name_hls_ready)\n",
    "showInNetron(model_name_hls_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model HLS layers conversion, need to fix depend on quantization type\n",
    "model = ModelWrapper(model_name_hls_ready)\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer())\n",
    "model.save(model_name_hls_layers)\n",
    "showInNetron(model_name_hls_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model create dataflow partition \n",
    "model = ModelWrapper(model_name_hls_layers)\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(model_name_data_flow)\n",
    "showInNetron(model_name_data_flow)\n",
    "\n",
    "#To show child dataflow layer that are wrap together in the parent graph\n",
    "#Use to check if process finished correctly\n",
    "sdp_node = getCustomOp(parent_model.graph.node[2]) #need to change here depend on the graph\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "showInNetron(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stream layers folding\n",
    "parent_model = ModelWrapper(model_name_data_flow)\n",
    "sdp_node = getCustomOp(parent_model.graph.node[2]) #need to change here depend on the graph\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "\n",
    "model = ModelWrapper(dataflow_model_filename)\n",
    "\n",
    "#Extract all StreamingFCLayer_Batch to assign folding, in the testing model is 4 layers\n",
    "fc0 = model.graph.node[0]\n",
    "fc1 = model.graph.node[1]\n",
    "fc2 = model.graph.node[2]\n",
    "fc3 = model.graph.node[3]\n",
    "\n",
    "fc0w = getCustomOp(fc0)\n",
    "fc1w = getCustomOp(fc1)\n",
    "fc2w = getCustomOp(fc2)\n",
    "fc3w = getCustomOp(fc3)\n",
    "\n",
    "#Set depend on paper experiment result and actual network\n",
    "fc0w.set_nodeattr(\"inFIFODepth\", 50)\n",
    "fc0w.set_nodeattr(\"SIMD\", 16)\n",
    "fc0w.set_nodeattr(\"PE\", 16)\n",
    "fc0w.set_nodeattr(\"outFIFODepth\", 4)\n",
    "\n",
    "fc1w.set_nodeattr(\"SIMD\", 16)\n",
    "fc1w.set_nodeattr(\"PE\", 16)\n",
    "fc1w.set_nodeattr(\"outFIFODepth\", 4)\n",
    "\n",
    "fc2w.set_nodeattr(\"SIMD\", 16)\n",
    "fc2w.set_nodeattr(\"PE\", 16)\n",
    "fc2w.set_nodeattr(\"outFIFODepth\", 4)\n",
    "\n",
    "fc3w.set_nodeattr(\"SIMD\", 16)\n",
    "fc3w.set_nodeattr(\"PE\", 10)\n",
    "fc3w.set_nodeattr(\"outFIFODepth\", 50)\n",
    "\n",
    "model = model.transform(InsertTLastMarker())\n",
    "model.save(model_name_set_folding)\n",
    "showInNetron(model_name_set_folding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HLS code generation and synthesis\n",
    "model = ModelWrapper(model_name_set_folding)\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(CodeGen_ipgen(fpga_part, target_clk_ns))\n",
    "\n",
    "model = model.transform(HLSSynth_IPGen())\n",
    "model.save(model_name_ipgen)\n",
    "showInNetron(model_name_ipgen)\n",
    "\n",
    "#For debugging\n",
    "# fc0w = getCustomOp(model.graph.node[0])\n",
    "# code_gen_dir = fc0w.get_nodeattr(\"code_gen_dir_ipgen\")\n",
    "# !ls {code_gen_dir}\n",
    "\n",
    "# shell_script = code_gen_dir + \"/ipgen.sh\"\n",
    "# !cat {shell_script}\n",
    "\n",
    "# tcl_script = code_gen_dir + \"/hls_syn_StreamingFCLayer_Batch_0.tcl\"\n",
    "# !cat {tcl_script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP stitching, a.k.a block design. This will create a vivado project with block condition that can be view and modified\n",
    "model = ModelWrapper(model_name_ipgen)\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(CodeGen_ipstitch(fpga_part))\n",
    "\n",
    "#model.model.metadata_props\n",
    "#model.get_metadata_prop(\"vivado_stitch_proj\")\n",
    "\n",
    "model.save(model_name_ipstitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PYNQ overlay\n",
    "model = ModelWrapper(model_name_ipstitch)\n",
    "model = model.transform(MakePYNQProject(pynq_board))\n",
    "\n",
    "#model.model.metadata_props\n",
    "#! ls {model.get_metadata_prop(\"vivado_pynq_proj\")}\n",
    "\n",
    "model.save(model_name_pynq_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bitstream synthesis, really really long\n",
    "model = ModelWrapper(model_name_pynq_proj)\n",
    "model = model.transform(SynthPYNQProject())\n",
    "\n",
    "#model.model.metadata_props\n",
    "\n",
    "model.save(model_name_post_synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from distutils.dir_util import copy_tree\n",
    "from shutil import copy\n",
    "\n",
    "from finn.transformation import Transformation\n",
    "from finn.util.basic import make_build_dir\n",
    "\n",
    "class DeployToPYNQCustom(Transformation):\n",
    "    \"\"\"Collects all necessary files for deployment and copies them to the PYNQ board.\n",
    "    Expects information about PYNQ board to make scp possible:\n",
    "    IP address of board, username and password for board and target directory where\n",
    "    the files are stored on the board\"\"\"\n",
    "\n",
    "    def __init__(self, target_dir):\n",
    "        super().__init__()\n",
    "        self.target_dir = target_dir\n",
    "\n",
    "    def apply(self, model):\n",
    "        # set metadata properties accordingly to user input specifications\n",
    "        model.set_metadata_prop(\"pynq_target_dir\", self.target_dir)\n",
    "\n",
    "        # create directory for deployment files\n",
    "        deployment_dir = make_build_dir(prefix=\"pynq_deployment_\")\n",
    "        model.set_metadata_prop(\"pynq_deployment_dir\", deployment_dir)\n",
    "\n",
    "        # get and copy necessary files\n",
    "        # .bit and .hwh file\n",
    "        vivado_pynq_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "        for file in os.listdir(vivado_pynq_proj):\n",
    "            if file.endswith(\".bit\"):\n",
    "                bitfile = os.path.join(vivado_pynq_proj, file)\n",
    "            elif file.endswith(\".hwh\"):\n",
    "                hwhfile = os.path.join(vivado_pynq_proj, file)\n",
    "        copy(bitfile, deployment_dir)\n",
    "        copy(hwhfile, deployment_dir)\n",
    "\n",
    "        # driver.py and python libraries\n",
    "        pynq_driver_dir = model.get_metadata_prop(\"pynq_driver_dir\")\n",
    "        copy_tree(pynq_driver_dir, deployment_dir)\n",
    "        model.set_metadata_prop(\"pynq_deploy_dir\", deployment_dir)\n",
    "        model.set_metadata_prop(\"exec_mode\", \"\")\n",
    "\n",
    "        return (model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver generation to load bitfile to Ultra96 and deploy to the board\n",
    "model = ModelWrapper(model_name_post_synthesis)\n",
    "model = model.transform(MakePYNQDriver())\n",
    "\n",
    "#model = model.transform(DeployToPYNQ(ip, username, password, target_dir))\n",
    "\n",
    "model = model.transform(DeployToPYNQCustom(target_dir))\n",
    "model.save(model_name_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check ssh connection, need to change to local executing mode\n",
    "model.model.metadata_props\n",
    "#! sshpass -p {password} ssh {username}@{ip} 'ls -l {target_dir}/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view drive.py code \n",
    "driver_dir = model.get_metadata_prop(\"pynq_driver_dir\")\n",
    "! cat {driver_dir}/driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre_processed data here for testing, will use Kynwhye scripts\n",
    "#load_dance_dance_data\n",
    "input_data = np.load(\"inputs.npy\")\n",
    "label_data = np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace StreamBatchLayer_wrapper with the deploy model\n",
    "x = input_data[0]\n",
    "\n",
    "parent_model = ModelWrapper(model_name_data_flow)\n",
    "sdp_node = parent_model.graph.node[2]\n",
    "local_exec_model = model_name_deploy\n",
    "getCustomOp(sdp_node).set_nodeattr(\"model\", local_exec_model)\n",
    "parent_model.save(model_name_deploy_integrated)\n",
    "\n",
    "#Executing through ssh connection\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "input_dict = {iname: input_data.reshape(ishape)}\n",
    "\n",
    "\n",
    "ret = execute_onnx(parent_model, input_dict, True) #rewrite this to increase speed of in/output\n",
    "prediction = ret[oname]\n",
    "\n",
    "print(prediction, labels[0])\n",
    "#Output function for classification task\n",
    "# def softmax(x):\n",
    "#     \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "#     return e_x / e_x.sum()\n",
    "\n",
    "# logits = ret[oname].flatten()\n",
    "# prob = softmax(logits)\n",
    "\n",
    "# plt.bar(np.arange(10), prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
